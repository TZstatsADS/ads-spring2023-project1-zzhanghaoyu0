#plot the top words
ggplot(all_wordCount,aes(x=tokenized,y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#plot the top words
ggplot(all_wordCount,aes(x=reorder(tokenized,tokenized,function(x)-length(x)),y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#plot the top words
ggplot(all_wordCount,aes(x=reorder(tokenized,tokenized,function(x)-length(x)),y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
ggtitle("Freqently mentioned word counts")
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
arrange(desc(n))
#plot the top words
ggplot(all_wordCount,aes(x=tokenized,y=n,fill=school))+
geom_bar(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#plot the top words
ggplot(all_wordCount,aes(x=tokenized,y=n,fill=school))+
geom_bar()+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#plot the top words
ggplot(all_wordCount,aes(x=tokenized,y=n,fill=school))+
geom_bar(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n))
#plot the top words
ggplot(all_wordCount,aes(x=word2,y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,school,n))
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n,.fun = mean))
#plot the top words
ggplot(all_wordCount,aes(x=word2,y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n,.fun = sum))
#plot the top words
ggplot(all_wordCount,aes(x=word2,y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n,.fun = max))
#plot the top words
ggplot(all_wordCount,aes(x=word2,y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = reorder(n))
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = reorder(tokenized,n))
#plot the top words
ggplot(all_wordCount,aes(x=word2,y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#count the words
all_wordCount <- all_school_tidy %>%
count(tokenized,school) %>%
group_by(school) %>%
top_n(10,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n))
#plot the top words
ggplot(all_wordCount,aes(x=word2,y=n,fill=school))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned word counts")
#what is their most frequently mentioned sentimental words
sentiment_all_tidy <- all_school_tidy %>% inner_join(get_sentiments("bing"),by=c("tokenized"="word"))
sentiment_all_tidy %>% count(tokenized,sentiment) %>%
spread(sentiment,n)
sentiment_wordCount <- sentiment_all_tidy %>%
count(tokenized,school,sentiment) %>%
group_by(school,sentiment) %>%
top_n(5,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n))
sentiment_all_wordCount <- sentiment_all_tidy %>%
count(tokenized,school,sentiment) %>%
group_by(school,sentiment) %>%
top_n(5,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n))
ggplot(sentiment_all_wordCount,aes(x=word2,y=n,fill=sentiment))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned sentimental word counts")
levels(factor(philosophy_data$original_publication_date))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
filter(school=="plato")
levels(factor(time_all$original_publication_date))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
filter(school=="aristotle")
levels(factor(time_all$original_publication_date))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
filter(school=="rationalism")
levels(factor(time_all$original_publication_date))
levels(factor(all_school$school))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
filter(school=="analytic")
levels(factor(time_all$original_publication_date))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
filter(school=="capitalism")
levels(factor(time_all$original_publication_date))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
mutatle(num_time = length(levels(factor(original_publication_date))))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
mutate(num_time = length(levels(factor(original_publication_date))))
time_all
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
mutate(num_time = length(levels(factor(original_publication_date))))
time_all
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school) %>%
ungroup() %>%
mutate(num_time = length(levels(factor(original_publication_date))))
time_all
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school) %>%
mutate(num_time = length(levels(factor(original_publication_date))))
time_all
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school,original_publication_date) %>%
mutate(num_time = length(levels(factor(original_publication_date))))
time_all
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school,original_publication_date) %>%
summarise(num_time = count(levels(factor(original_publication_date))))
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school,original_publication_date) %>%
summarise(num_time = length(levels(factor(original_publication_date))))
time_all
View(time_all)
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school) %>%
summarise(num_time = length(levels(factor(original_publication_date))))
time_all
#re-select data to include time
time_all <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school) %>%
summarise(num_time = length(levels(factor(original_publication_date)))) %>%
top_n(5,num_time)
time_all
#re-select data to include time
time_count <- philosophy_data %>%
select(school,original_publication_date) %>%
group_by(school) %>%
summarise(num_time = length(levels(factor(original_publication_date)))) %>%
top_n(5,num_time)
time_count
time_tidy <- philosophy_data %>%
select(school,original_publication_date,tokenized_txt) %>%
filter(school %in% c("analytic","german_idealism","continental"))
time_tidy$text <- time_tidy$tokenized_txt %>%
removePunctuation()
time_tidy <- time_tidy %>% select(school,text,original_publication_date)
#create one word one row tokenized data
time_tidy <- time_tidy %>% unnest_tokens(tokenized,text)
head(time_tidy)
#calculate polarity and make plot against time
time_polarity <- time_tidy %>%
inner_join(bing, by = c("tokenized" = "word")) %>%
count(sentiment, original_publication_date) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(
polarity = positive - negative
)
#calculate polarity and make plot against time
time_polarity <- time_tidy %>%
inner_join(get_sentiments("bing"), by = c("tokenized" = "word")) %>%
count(sentiment, original_publication_date) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(
polarity = positive - negative
)
time_polarity
#calculate polarity and make plot against time
time_polarity <- time_tidy %>%
inner_join(get_sentiments("bing"), by = c("tokenized" = "word")) %>%
count(sentiment, school,original_publication_date) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(
polarity = positive - negative
)
time_polarity
# Add a plot title
ggtitle("Moby Dick Chronological Polarity") +
theme_gdocs()
# Add a plot title
ggtitle("Moby Dick Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity)) +
# Add a smooth trend curve
geom_smooth() +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_y")+
# Add a plot title
ggtitle("Moby Dick Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity)) +
# Add a smooth trend curve
geom_smooth() +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Moby Dick Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth() +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Moby Dick Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth() +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth(show.legend = FALSE) +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth(show.legend = FALSE,se=FALSE) +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Chronological Polarity")
#continental philosophy has a shap positive polarity around 1965
#further observe the sentimental words to make sure they are indeed positive
continental_1965 <- time_tidy %>%
filter(school=="continental",
original_publication_date==1965)
continental_1965
time_count
#continental philosophy has a shap positive polarity around 1965
#further observe the sentimental words to make sure they are indeed positive
continental_1965 <- time_tidy %>%
filter(school=="continental")
continental_1965
#remove stop words
time_tidy <- time_tidy %>% anti_join(stop_words,by=c("tokenized"='word'))
#calculate polarity and make plot against time
time_polarity <- time_tidy %>%
inner_join(get_sentiments("bing"), by = c("tokenized" = "word")) %>%
count(sentiment, school,original_publication_date) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(
polarity = positive - negative
)
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth(show.legend = FALSE,se=FALSE) +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Chronological Polarity")
#continental philosophy has a shap positive polarity around 1965
#further observe the sentimental words to make sure they are indeed positive
continental <- time_tidy %>%
filter(school=="continental")
continental
levels(factor(continental$original_publication_date))
#the positive section happened in 1963 to 1967
continental_63to67 <- continental %>%
filter(original_publication_date %in% c(1963,1967))
continental_63to67
#count the sentimental words used
continental_63to67_sentiment <- continental_63to67 %>% inner_join(get_sentiments("bing"),by=c("tokenized"="word"))
continental_63to67_sentiment <- continental_63to67_sentiment %>%
count(tokenized,school,sentiment) %>%
group_by(school,sentiment) %>%
top_n(5,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n))
continental_63to67_sentiment
ggplot(continental_63to67_sentiment,aes(x=word2,y=n,fill=sentiment))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned sentimental word counts")
#the positive section happened in 1963 to 1967
continental_63to67 <- continental %>%
filter(original_publication_date %in% c(1963,1966,1967))
#count the sentimental words used
continental_63to67_sentiment <- continental_63to67 %>% inner_join(get_sentiments("bing"),by=c("tokenized"="word"))
continental_63to67_sentiment <- continental_63to67_sentiment %>%
count(tokenized,school,sentiment) %>%
group_by(school,sentiment) %>%
top_n(5,n) %>%
ungroup() %>%
mutate(word2 = fct_reorder(tokenized,n))
ggplot(continental_63to67_sentiment,aes(x=word2,y=n,fill=sentiment))+
geom_col(show.legend = FALSE)+
facet_wrap(~school,scales = "free_y")+
coord_flip()+
ggtitle("Freqently mentioned sentimental word counts")
filter(school == "continental",
original_publication_date %in% c(1963,1966,1967))
#the word pure are ambiguous like pure evil
#so look for sentences contain these two words
continental_sentences <- philosophy_data %>%
select(school,original_publication_date,sentence_lowered) %>%
filter(school == "continental",
original_publication_date %in% c(1963,1966,1967))
continental_sentences
head(continental_sentences)
continental_sentences[grep("pure",continental_sentences$sentence_lowered),]
#the word pure are ambiguous like pure evil
#so look for sentences contain these two words
continental_sentences <- philosophy_data %>%
select(school,sentence_lowered) %>%
filter(school == "continental",
original_publication_date %in% c(1963,1966,1967))
select(school,sentence_lowered)
#the word pure are ambiguous like pure evil
#so look for sentences contain these two words
continental_sentences <- philosophy_data %>%
filter(school == "continental",
original_publication_date %in% c(1963,1966,1967)) %>%
select(school,sentence_lowered)
head(continental_sentences)
continental_sentences[grep("pure",continental_sentences$sentence_lowered),]
#the word pure are ambiguous like pure evil
#so look for sentences contain these two words
continental_sentences <- philosophy_data %>%
filter(school == "continental",
original_publication_date %in% c(1963,1966,1967)) %>%
select(sentence_lowered)
head(continental_sentences)
continental_sentences[grep("pure",continental_sentences$sentence_lowered),]
#try add pure as a stop word and re plot
custom_stop_words <- tribble(
~word,~lexicon,
"pure","CUSTOM"
)
stop_words_new <- stop_words %>% bind_rows(custom_stop_words)
time_tidy_new <- time_tidy %>% anti_join(stop_words_new,by=c("tokenized"='word'))
time_polarity_new <- time_tidy_new %>%
inner_join(get_sentiments("bing"), by = c("tokenized" = "word")) %>%
count(sentiment, school,original_publication_date) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(
polarity = positive - negative
)
# Plot polarity vs. line_number
ggplot(time_polarity_new, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth(show.legend = FALSE,se=FALSE) +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth(show.legend = FALSE,se=FALSE) +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Chronological Polarity")
# Plot polarity vs. line_number
ggplot(time_polarity_new, aes(x=original_publication_date,y=polarity,fill=school)) +
# Add a smooth trend curve
geom_smooth(show.legend = FALSE,se=FALSE) +
# Add a horizontal line at y = 0
geom_hline(yintercept = 0, color = "red") +
facet_wrap(~school,scales = "free_x")+
# Add a plot title
ggtitle("Chronological Polarity")
head(philosophy_data)
#examine how many different schools does this data set contain
levels(factor(philosophy_data$school))
#examine how many different titles does this dataset contain
levels(factor(philosophy_data$title))
knitr::opts_chunk$set(echo = TRUE)
#load the dataset
philosophy_data <- read.csv("D:/source/Stats 5243/philosophy_data.csv")
levels(factor(philosophy_data$school))
as.matrix(levels(factor(philosophy_data$school)))
as.matrix(levels(factor(philosophy_data$school)),ncol=4)
matrix(levels(factor(philosophy_data$school)),ncol=4)
matrix(levels(factor(philosophy_data$school)))
#names of the 13 philosophy schools
levels(factor(philosophy_data$school))
knitr::include_graphics("../figs/data.png")
which(is.null(philosophy_data))
sum(is.null(philosophy_data))
data.frame(c(1,2,3),ncol=1)
data.frame(index = c(1,2,3),column_names=c("school","original_publication_date","tokenized_txt"))
data.frame(index = c(1,2,3),column_names=c("school","original_publication_date","tokenized_txt")
column_content=c("The philosophy school the text corresponds to",
data.frame(index = c(1,2,3),column_names=c("school","original_publication_date","tokenized_txt"),
column_content=c("The philosophy school the text corresponds to",
"The original publication year of the text",
"The tokenized text"))
